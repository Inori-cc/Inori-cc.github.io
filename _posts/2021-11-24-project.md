---
layout: post
title:  "Final Project"
date:   2021-11-24
excerpt: "Prediction of Patient's Risk of Death based on Lab Test after Entering ICU"
tag: [post]
comments: true
---


##  Prediction of Patient's Risk of Death based on Lab Test after Entering ICU

Team Name: Obliviate
Team Members: Ruqian Cheng, Mingxuan Wang, Ruiqi Zhang

Product Web: <https://streamlit-app823.herokuapp.com/>  
Project Blog: <https://obliviate823.github.io/>  
Github Repository: <https://github.com/Obliviate823/823_FinalProject>

## Dataset

[MIMIC](https://mimic.mit.edu/) dataset is chosen. This dataset includes the unidentified health data related to approximately 60,000 ICU admitted patients (53,432 adult patients and 8,100 neonatal patients) from June 2001 to October 2012. The data used include variables from LABEVENTS.csv, ICUSTAYS.csv, ADMISSIONS.csv, D_LABITEMS.csv and PATIENTS.csv.

Outcome variable (binary):

- `flag`: alive or death.

Predictor variables:

- `Base Excess`: the amount of excess or insufficient level of bicarbonate in the system.
- `Anion Gap`: a measurement of the difference-or gap-between the negatively charged and positively charged electrolytes.
- `Chloride`: measures the amount of chloride in your blood.
- `Creatinine`: a chemical compound left over from energy-producing processes in your muscles.
- `Potassium`: measures the amount of potassium in your blood.
- `Sodium`: measures the amount of sodium in your blood.
- `Urea Nitrogen`: reveals important information about how well your kidneys are working.
- `Red Cell Distribution Width (RDW)`: a measurement of the range in the volume and size of your red blood cells.
- `White Blood Cells`: measures the number of white blood cells in your body.
- `icu_los`: ICU length of stay

## Object
The aim of our project is to predict the risk of death based on blood test after entering the ICU. To achieve it, our work mainly includes three parts:
- Data Preprocessing
- ML Algorithm
- Web Application


## 1.  Data Preprocessing

### Library

```
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
```
### Load datasets
```
icustays = pd.read_csv('ICUSTAYS.csv')
labevents = pd.read_csv('LABEVENTS.csv')
admissions = pd.read_csv('ADMISSIONS.csv')
d_labitems = pd.read_csv('D_LABITEMS.csv')
patients = pd.read_csv('PATIENTS.csv')
```
### Icustays Processing
Firstly, for icustays processing, we obtain the time of each patient entering and leaving ICU for the first time, and the length of stay in ICU.

Change the "INTIME" and "OUTTIME" to datetime format, sort the icustays datafrmae by "INTIME".
```
icustays['INTIME'] = pd.to_datetime(icustays['INTIME'])
icustays['OUTTIME'] = pd.to_datetime(icustays['OUTTIME'])
icustays.sort_values('INTIME', inplace = True)
```
Delete rows with missing values in column "OUTTIME".
```
icustays.drop(icustays[np.isnan(icustays['OUTTIME'])].index, axis = 0, inplace = True)
```
Obtain the intime, outtime for each patient to enter the icu for the first time. Due to the sorting of intime, the data in the first row of each patient is the data entered into ICU for the first time.
```
time_dict = {}
for i in icustays.index:
    if icustays['SUBJECT_ID'][i] not in time_dict.keys():
        time_dict[icustays['SUBJECT_ID'][i]] = {'INTIME' : icustays['INTIME'][i], 
                                                'OUTTIME' : icustays['OUTTIME'][i]}
```
### Labevents processing
Secondly, we implement labevents processing. Each row of the datasets represents the value of a certain item tested by a certain patient. A patient may have multiple tests for one item. For a certain item, we want to obtain the value of the patient's first test during the first time entering ICU.

Only keep patients who have been in ICU
```
labevents_icu = labevents[[(sub_id in time_dict.keys()) for sub_id in labevents['SUBJECT_ID']]]
```
Change the "CHARTTIME" to datetime format, sort the labevents_icu datafrmae by "INTIME".
```
labevents_icu['CHARTTIME'] = pd.to_datetime(labevents_icu['CHARTTIME'])
labevents_icu.sort_values('CHARTTIME', inplace = True)
```
For every patient and every item, obtain the first testing value during his first entry into ICU.
```
%%time

itemid_dict = {}
require_index = {}
for i in labevents_icu.index:
    if labevents_icu['CHARTTIME'][i] > time_dict[labevents_icu['SUBJECT_ID'][i]]['INTIME'] and labevents_icu['CHARTTIME'][i] < time_dict[labevents_icu['SUBJECT_ID'][i]]['OUTTIME']:
        if labevents_icu['SUBJECT_ID'][i] not in itemid_dict.keys():
            itemid_dict[labevents_icu['SUBJECT_ID'][i]] = []
        if labevents_icu['ITEMID'][i] not in itemid_dict[labevents_icu['SUBJECT_ID'][i]]:
            itemid_dict[labevents_icu['SUBJECT_ID'][i]].append(labevents_icu['ITEMID'][i])
            require_index[i] = 0
```
```
>>> CPU times: user 13min 9s, sys: 4.19 s, total: 13min 13s
>>> Wall time: 13min 17s
```
Select required rows:
```
labevents_unique = labevents_icu[[(index in require_index.keys()) for index in labevents_icu.index]]
```
Obtain the HADM_ID dict, in labevents_unqiue, for each patient, the HADM_ID should be unique.
```
HADM_dict = {}
for i in labevents_unique.index:
    if labevents_unique['SUBJECT_ID'][i] not in HADM_dict.keys():
        HADM_dict[labevents_unique['SUBJECT_ID'][i]] = labevents_unique['HADM_ID'][i]
```
Since the labevents are unique for each subject, we can use pd.pivot to convert the labevents_unique dataframe.

Convert labevents_unique:
```
convert = labevents_unique.pivot(index='SUBJECT_ID',columns='ITEMID',values='VALUE')
```
Since there are too many nan in convert, we want to keep the columns with missing values less than 20000, then delete the rows that has missing values in these columns.
```
drop_list = []
for column in convert.columns:
    if sum(pd.isna(convert[column])) > 20000:
        drop_list.append(column)
convert_dropna = convert.drop(drop_list, axis = 1).dropna(axis = 0, how = 'any')
```
Some of the value cannot be transfered to float, therefore, we want to delete the rows with these string values.
```
def isfloat(i):
    try:
        float(i)
        return True
    except:
        return False
    
drop_index = []
for i in convert_dropna.index:
    for column in convert_dropna.columns:
        if not isfloat(convert_dropna[column][i]):
            drop_index.append(i)
            
convert_float = convert_dropna.drop(drop_index,axis = 0)
```
### Admissions processing
We obtain the target variable and Admittime from admissions datasets.

Obtain the HADM_ID, flag, admittime for each patient:
```
admissions['ADMITTIME'] = pd.to_datetime(admissions['ADMITTIME'])
admissions = admissions.sort_values('ADMITTIME')
HADM_dict = {}
admittime = {}
flag_dict = {}
for i in admissions.index:
    if admissions['SUBJECT_ID'][i] not in HADM_dict.keys():
        HADM_dict[admissions['SUBJECT_ID'][i]] = admissions['HADM_ID'][i]
        admittime[admissions['SUBJECT_ID'][i]] = admissions['ADMITTIME'][i]
        flag_dict[admissions['SUBJECT_ID'][i]] = admissions['HOSPITAL_EXPIRE_FLAG'][i]
```
### Patients processing
This dataset includes patients information, we want to get the date of birthday and gender for each patient.
```
patients['GENDER'] = patients['GENDER'].replace({'F':0, 'M':1})
patients['DOB'] = pd.to_datetime(patients['DOB'])
```
Obtain the gender_dict and age_dict:
```
gender_dict = {}
age_dict = {}
for i in patients.index:
    gender_dict[patients['SUBJECT_ID'][i]] = patients['GENDER'][i]
    age_dict[patients['SUBJECT_ID'][i]] = admittime[patients['SUBJECT_ID'][i]].year - patients['DOB'][i].year
```
### Combination
Based on subject_id, combine the variable we get.

Based on the HADM_dict obtained from admissions, obtain icu_los from icustays dataframe. One hospitalization may enter the ICU multiple times, and the length needs to be accumulated
```
icu_los = {}
for i in icustays.index:
    if icustays['HADM_ID'][i] == HADM_dict[icustays['SUBJECT_ID'][i]]:
        if icustays['SUBJECT_ID'][i] not in icu_los.keys():
            icu_los[icustays['SUBJECT_ID'][i]] = 0
        icu_los[icustays['SUBJECT_ID'][i]] += icustays['LOS'][i]
```
```
combine = convert_float[[(index in icu_los.keys()) for index in convert_float.index]]
```
```
combine['age'] = -1.0
combine['gender'] = -1
combine['icu_los'] = -1.0
combine['flag'] = -1

for i in combine.index:
    combine.age[i] = age_dict[i]
    combine.gender[i] = gender_dict[i]
    combine.icu_los[i] = time_dict[i]['icu_los']
    combine.flag[i] = flag_dict[i]
```
### Variables selection
There are too many variables, we want to choose 10 variables for the next part of data analysis. Therefore, we use a simple logistic regression to select variables.

Map the data to the range of 0-1:
```
combine_nor = combine.copy()
for column in combine_nor.columns:
    combine_nor[column] = combine_nor[column].astype('float') - min(combine_nor[column].astype('float'))
    combine_nor[column] = combine_nor[column] / max(combine_nor[column])
```
Logistic regression:
```
x_column = combine_nor.columns[:-1]
x = combine_nor[x_column]
y = combine_nor['flag']
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)
clf = LogisticRegression(random_state=0, class_weight = 'balanced').fit(X_train, y_train)
clf.score(X_test,y_test)
```
```
>>> 0.6987647389107243
```
The importance of variables:
```
clf.coef_
```
```
>>> array([[-2.1758029 , -2.73706951,  1.55998412,  0.54184627, -0.3229467 ,
         3.59556658, -0.37894219, -1.12112914, -4.30788353, -4.252456  ,
         1.81547318, -1.923065  , -0.50033385, -2.18868785,  3.00685438,
         2.89995361, -0.29202966,  0.66445185,  1.32347575,  1.32839141,
        -2.36666072,  1.98036039, -0.87288612,  1.21127145,  0.69200924,
         4.90876271,  0.67380674,  2.69264174,  1.2553999 , -0.0210873 ,
         1.73804051]])
```
```
x_column
```
```
>>> Index([    50802,     50804,     50818,     50820,     50821,     50868,
           50882,     50893,     50902,     50912,     50931,     50960,
           50970,     50971,     50983,     51006,     51221,     51222,
           51237,     51248,     51249,     51250,     51265,     51274,
           51275,     51277,     51279,     51301,     'age',  'gender',
       'icu_los'],
      dtype='object', name='ITEMID')
```
Drop the variables with low importance:
```
drop_cols = [50820, 50821, 50882, 50970, 51221, 51222, 
             51265, 51275, 51279, 50818, 50893, 50804,
             51274, 51250, 51249, 51248, 51237, 50931, 
             50960,'gender','age']

combine_new = combine.drop(drop_cols, axis = 1)
```
Map the data to the range of 0-1:
```
combine_nor = combine_new.copy()
for column in combine_nor.columns:
    combine_nor[column] = combine_nor[column].astype('float') - min(combine_nor[column].astype('float'))
    combine_nor[column] = combine_nor[column] / max(combine_nor[column])
```
Repeat the logistic regression:
```
x_column = combine_nor.columns[:-1]
x = combine_nor[x_column]
y = combine_nor['flag']
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)
clf = LogisticRegression(random_state=0, class_weight = 'balanced').fit(X_train, y_train)
clf.score(X_test,y_test)
```
```
>>> 0.7063447501403706
```
```
clf.coef_
```
```
>>> array([[-4.95377852,  4.10662256, -3.96803642, -4.65643424, -2.22710977,
         3.08866454,  3.54293   ,  5.54482311,  2.77263174,  1.53964616]])

```
We can see that these ten variables have high importance. Therefore, we choose these ten variables for the next part of data analysis.

### Convert itemid and save dataframe
Transfer item id to item name:
```
itemid_dict = {}
for i in d_labitems.index:
    if d_labitems['ITEMID'][i] in combine_new.columns:
        itemid_dict[d_labitems['ITEMID'][i]] = d_labitems['LABEL'][i]
        
final_dataset = combine_new.rename(columns = itemid_dict)
final_dataset
```
Save to csv file:
```
final_dataset.to_csv('death_risk_predict.csv')
```

## 2. ML Algorithm
## Data Analysis Part One
This part we will use logistic regression and SVM model to analyze the dataset we get from Data Processing.

### Import Libraries
```
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn import svm
import joblib
import matplotlib.pyplot as plt
import warnings
```
```
data = pd.read_csv('death_risk_predict.csv', index_col = 0)
```
Normalization:
```
min_dict = {}
max_min_dict = {}
data_nor = data.copy()
for column in data.columns:
    min_dict[column] = min(data_nor[column])
    data_nor[column] = data_nor[column] - min(data_nor[column])
    max_min_dict[column] = max(data_nor[column])
    data_nor[column] = data_nor[column] / max(data_nor[column])
```
Split training dataset and test dataset:
```
x_column = data_nor.columns[1:-1]
x = data_nor[x_column]
y = data_nor['flag']
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=273)
```
### - Logistic Regression
The target variable is unbanlaced, so we need to set a class_weight.

The proportion of 1:
```
sum(y)/len(y)
```
```
>>> 0.17423774496041328
```
Use {1:0.8, 0:0.2} as class_weight:
```
clf = LogisticRegression(random_state=273, class_weight = {1:0.8, 0:0.2}).fit(X_train, y_train)
y_logis_predict = clf.predict(X_test)
y_logis_predict_proba = [prob[1] for prob in clf.predict_proba(X_test)]
accu_logis = metrics.accuracy_score(y_test,y_logis_predict)
auc_logis = metrics.roc_auc_score(y_test,y_logis_predict_proba)
print('Logistic Regression:')
print('Accuracy on testing data:', accu_logis)
```
```
>>> Logistic Regression:
Accuracy on testing data: 0.776530039303762
AUC on testing data: 0.7701827348053764
```
ROC curve:
```
fpr_logis, tpr_logis, thresholds_logis = metrics.roc_curve(y_test, y_logis_predict_proba)

plt.plot(fpr_logis, tpr_logis, label='ROC curve (area = %0.2f)' % auc_logis)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc="lower right")
plt.title('ROC for Linear SVM')
plt.show()
```
![Image 1](/v_source/final-1.png)

### - SVM
### Linear kernel
```
linear_svm = svm.SVC(class_weight = {1:0.8, 0:0.2},kernel = 'linear', probability = True)
linear_svm.fit(X_train, y_train)
```
```
>>> SVC(class_weight={0: 0.2, 1: 0.8}, kernel='linear', probability=True)
```
```
y_linear_predict = linear_svm.predict(X_test)
y_linear_predict_proba = [prob[1] for prob in linear_svm.predict_proba(X_test)]
accu_linear = metrics.accuracy_score(y_test, y_linear_predict)
auc_linear = metrics.roc_auc_score(y_test, y_linear_predict_proba)
print('Linear SVM:')
print('Accuracy on testing data:', accu_linear)
print('AUC on testing data:', auc_linear)
```
```
>>> Linear SVM:
Accuracy on testing data: 0.7916900617630545
AUC on testing data: 0.7690908410248032
```
ROC curve:
```
fpr_linear, tpr_linear, thresholds_linear = metrics.roc_curve(y_test, y_linear_predict_proba)

plt.plot(fpr_linear, tpr_linear, label='ROC curve (area = %0.2f)' % auc_linear)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc="lower right")
plt.title('ROC for Linear SVM')
plt.show()
```
![Image 2](/v_source/final-2.png)

### Polynomial kernal
```
poly_svm = svm.SVC(class_weight = {1:0.8, 0:0.2},kernel = 'poly',probability = True)
poly_svm.fit(X_train, y_train)
```
```
>>> SVC(class_weight={0: 0.2, 1: 0.8}, kernel='poly', probability=True)
```
```
y_poly_predict = poly_svm.predict(X_test)
y_poly_predict_proba = [prob[1] for prob in poly_svm.predict_proba(X_test)]
accu_poly = metrics.accuracy_score(y_test, y_poly_predict)
auc_poly = metrics.roc_auc_score(y_test, y_poly_predict_proba)
fpr_poly, tpr_poly, thresholds_poly = metrics.roc_curve(y_test, y_poly_predict_proba)
print('Polynomial SVM:')
print('Accuracy on testing data:', accu_poly)
print('AUC on testing data:', auc_poly)
```
```
>>> Polynomial SVM:
Accuracy on testing data: 0.7821448624368332
AUC on testing data: 0.7853634616606315
```
ROC curve:
```
fpr_poly, tpr_poly, thresholds_poly = metrics.roc_curve(y_test, y_poly_predict_proba)

plt.plot(fpr_poly, tpr_poly, label='ROC curve (area = %0.2f)' % auc_poly)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc="lower right")
plt.title('ROC for Polynomial SVM')
plt.show()
```
![Image 3](/v_source/final-3.png)

### Radial Basis Function Kernel
```
rbf_svm = svm.SVC(class_weight = {1:0.8, 0:0.2},kernel = 'rbf', gamma = 1/1000, probability = True)
rbf_svm.fit(X_train, y_train)
```
```
>>> SVC(class_weight={0: 0.2, 1: 0.8}, gamma=0.001, probability=True)
```
```
y_rbf_predict = rbf_svm.predict(X_test)
y_rbf_predict_proba = [prob[1] for prob in rbf_svm.predict_proba(X_test)]
accu_rbf = metrics.accuracy_score(y_test, y_rbf_predict)
auc_rbf = metrics.roc_auc_score(y_test, y_rbf_predict_proba)
print('Radial Basis Function SVM:')
print('Accuracy on testing data:', accu_rbf)
print('AUC on testing data:', auc_rbf)
```
```
>>> Radial Basis Function SVM:
Accuracy on testing data: 0.8332397529477822
AUC on testing data: 0.7507546829480791
```
ROC curve:
```
fpr_rbf, tpr_rbf, thresholds_rbf = metrics.roc_curve(y_test, y_rbf_predict_proba)

plt.plot(fpr_rbf, tpr_rbf, label='ROC curve (area = %0.2f)' % auc_rbf)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc="lower right")
plt.title('ROC for Radial Basis Function SVM')
plt.show()
```
![Image 4](/v_source/final-4.png)

### Model Selection and Explanation
We want to predict the death risk, so we should choose the model with highest AUC. Therefore, we will choose polynomial kernel SVM model.
```
import shap
explainer = shap.KernelExplainer(poly_svm.predict_proba, shap.sample(X_train, 50), link="logit")
shap_values = explainer.shap_values(X_test, nsamples=50)
```
```
shap.initjs()
shap.force_plot(explainer.expected_value[0], shap_values[0], X_test)
```
![Image 5](/v_source/final-5.png)

We can see that for variables "Urea Nitrogen", "Base Excess", "Creatinine", "icu_los", their increase usually increase the risk of death. And for varialbes "RDW", "Anion Gap", "White Blood Cells", their increase usually decrease the risk of death.

### Model Function for Web Application
Save the model:
```
joblib.dump(poly_svm, 'poly_svm_model.joblib')
['poly_svm_model.joblib']
```
Load the model:
```
model = joblib.load('poly_svm_model.joblib')
```
Predict function:
```
def death_risk_predict(Base_Excess, Anion_Gap, Chloride, Creatinine, Potassium, Sodium, Urea_Nitrogen, RDW, White_Blood_Cells, icu_los):
    
    # Normalization
    x1 = (Base_Excess - min_dict['Base Excess']) / max_min_dict['Base Excess']
    x2 = (Anion_Gap - min_dict['Anion Gap']) / max_min_dict['Anion Gap']
    x3 = (Chloride - min_dict['Chloride']) / max_min_dict['Chloride']
    x4 = (Creatinine - min_dict['Creatinine']) / max_min_dict['Creatinine']
    x5 = (Potassium - min_dict['Potassium']) / max_min_dict['Potassium']
    x6 = (Sodium - min_dict['Sodium']) / max_min_dict['Sodium']
    x7 = (Urea_Nitrogen - min_dict['Urea Nitrogen']) / max_min_dict['Urea Nitrogen']
    x8 = (RDW - min_dict['RDW']) / max_min_dict['RDW']
    x9 = (White_Blood_Cells - min_dict['White Blood Cells']) / max_min_dict['White Blood Cells']
    x10 = (icu_los - min_dict['icu_los']) / max_min_dict['icu_los']
    
    # Predict
    death_risk = model.predict_proba(np.array([x1, x2, x3, x4, x5, x6, 
                                                  x7, x8, x9, x10]).reshape(1,-1))[0][1]
    
    return round(death_risk,4)
```
Subject_id = 3:
```
death_risk_predict(-4,23,111,2.4,4.0,143,41,15,11.3,6.0646)
```
```
>>> 0.3376
```
Subject_id = 12:
```
death_risk_predict(-21,28,111,1.3,4.6,145,28,14.2,8.4,7.6348)
```
```
>>> 0.6882
```
Save predict_proba:
```
roc = pd.DataFrame({'y_test':y_test, 
                    'y_logis_predict_proba':y_logis_predict_proba, 
                    'y_linear_predict_proba':y_linear_predict_proba, 
                    'y_poly_predict_proba':y_poly_predict_proba, 
                    'y_rbf_predict_proba':y_rbf_predict_proba})

roc.to_csv('ROC_logis_svm.csv',index = False)
```
Plot ROC function for web application:
```
ROC_logis_svm = pd.read_csv('ROC_logis_svm.csv')
AUC_dict = {'Logistic': 0.77,
            'Linear_SVM':0.77,
            'Poly_SVM':0.79,
            'RBF_SVM':0.75}

def ROC_plot(model_name):
    if model_name == 'Logistic':
        y_predict_proba = ROC_logis_svm['y_logis_predict_proba']
    elif model_name == 'Linear_SVM':
        y_predict_proba = ROC_logis_svm['y_linear_predict_proba']
    elif model_name == 'Poly_SVM':
        y_predict_proba = ROC_logis_svm['y_poly_predict_proba']
    elif model_name == 'RBF_SVM':
        y_predict_proba = ROC_logis_svm['y_rbf_predict_proba']
        
    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predict_proba)
    
    plt.plot(fpr_rbf, tpr_rbf, label='ROC curve (area = %0.2f)' % AUC_dict[model_name])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc="lower right")
    plt.title('ROC for Radial Basis Function SVM')
    plt.show()
```
```   
ROC_plot('Logistic')
```
![Image 6](/v_source/final-6.png)

## Data Analysis Part Two
```
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import sklearn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```
Read data:
```
dt = pd.read_csv("C:/1_Work/Duke_3rd_semester/823/Final Project/death_risk_predict.csv")
```
Train test split:
```
from sklearn.model_selection import train_test_split

X = dt.drop(columns=['Unnamed: 0','flag'])
y = dt['flag']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)
```
```
y_train.value_counts()
```
```
>>> 
0    11781
1     2466
Name: flag, dtype: int64
```
### Function of calculating AUC and plot ROC
```
def show_auc_roc(y_test,predicted_prob):
    '''
    calculating AUC and plot ROC
    '''
    predicted_prob=predicted_prob[:,1]

    a = float(sum(y_test))
    b = float(len(y_test)-sum(y_test))
    auc = (sum(np.array(predicted_prob).reshape(1,-1).argsort().argsort()[np.array(y_test).reshape(1,-1) == 1]+1) - a*(a+1)/2) / (a*b)
    print("Testing AUC = ",auc)
    
    tpr=np.array([])
    fpr=np.array([])
    for s in np.arange(max(predicted_prob),min(predicted_prob),-0.01):
        predicted_label=(predicted_prob>s).astype(int)
        tpr = np.append(tpr,np.sum((predicted_label==y_test) & (y_test==1))/np.sum(y_test==1))
        fpr = np.append(fpr,np.sum((predicted_label!=y_test) & (y_test==0))/np.sum(y_test==0))

    display = sklearn.metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc = auc, estimator_name = "Test Roc")
    display.plot()
```
### - Random Forest
Build a model without tuning:
```
rf_ori = RandomForestClassifier(random_state=0)
rf_ori = rf_ori.fit(X_train,y_train)
predicted_prob = rf_ori.predict_proba(X_test)
```
Call show_auc_roc() to calculate AUC and plot ROC:
```
show_auc_roc(y_test, predicted_prob)
```
![Image 7](/v_source/final-7.png)

### Tuning Process - Grid Searc
```
from sklearn.model_selection import (
    RandomizedSearchCV, 
    GridSearchCV
)
```
Write a function to show grid search result:
```
def show_gridsearch_result(gsearch_fit):
    """
    print and plot parameter and auc score during grid search
    """
    # output parameter and auc score
    print(np.column_stack([gsearch_fit.cv_results_['params'],gsearch_fit.cv_results_['mean_test_score']]))
    print(f"best parameter is: {gsearch_fit.best_params_}, best score is: {gsearch_fit.best_score_}")
    # plot parameter and auc score
    parameters = []
    scores=[]
    for parameter in gsearch_fit.cv_results_['params']:
        for value in parameter.values():
            parameters.append(value)
    for result in gsearch_fit.cv_results_['mean_test_score']:
        scores.append(result)
    if len(gsearch_fit.cv_results_['params'][0]) == 1:
        plt.plot(parameters,scores)
```
### Tuning n_estimators:
In random forest, when n_estimators increases, the AUC of the model is nearly monotonically increasing, so we tune n_estimators first.
```
# tuning process with grid search
param_test1= {'n_estimators':range(20,501,20)}
gsearch1= GridSearchCV(estimator = RandomForestClassifier(random_state=0),
                       param_grid =param_test1, scoring='roc_auc',cv=5)
gsearch1.fit(X_train,y_train)
# call function show_gridsearch_result() to print and plot parameter and auc score
show_gridsearch_result(gsearch1)
```
```
>>> GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0),
             param_grid={'n_estimators': range(20, 501, 20)},
             scoring='roc_auc')
```
Call function show_gridsearch_result() to print and plot parameter and auc score:
```
show_gridsearch_result(gsearch1)
```
```
>>> [[{'n_estimators': 20} 0.7296351612840247]
 [{'n_estimators': 40} 0.7457103097992073]
 [{'n_estimators': 60} 0.7503827808803869]
 [{'n_estimators': 80} 0.7540820485005111]
 [{'n_estimators': 100} 0.7554439815531927]
 [{'n_estimators': 120} 0.756586918540904]
 [{'n_estimators': 140} 0.7574440154946258]
 [{'n_estimators': 160} 0.7586053782720344]
 [{'n_estimators': 180} 0.7592983072293024]
 [{'n_estimators': 200} 0.7591691884245353]
 ...
 best parameter is: {'n_estimators': 460}, best score is: 0.7619269281478542
 ```
![Image 8](/v_source/final-8.png)

The AUC score becomes stable since n_estimators=400, then slightly increases as the n_estimators increases. To secure a high auc while avoid the problem of consuming too much training time, we choose n_estimators=400.

### Tuning tree-depth related parameters
Since parameter 'max_depth', 'min_samples_split', 'min_samples_leaf' are correlated, we tune these 3 parameters together using grid search.

Tuning process with grid search:
```
param_test2 = {
    'max_depth': range(7,19,2),
    'min_samples_split': range(5,21,5),
    'min_samples_leaf': range(4,15,2)
}
```
```
>>> gsearch2= GridSearchCV(estimator = RandomForestClassifier(n_estimators=400,random_state=0),
                       param_grid =param_test2, scoring='roc_auc',cv=5)
```
```
gsearch2.fit(X_train,y_train)
GridSearchCV(cv=5,
             estimator=RandomForestClassifier(n_estimators=400, random_state=0),
             param_grid={'max_depth': range(7, 19, 2),
                         'min_samples_leaf': range(4, 15, 2),
                         'min_samples_split': range(5, 21, 5)},
             scoring='roc_auc')
```
Call function show_gridsearch_result() to print and plot parameter and auc score:
```
show_gridsearch_result(gsearch2)
```
```
>>> [[{'max_depth': 7, 'min_samples_leaf': 4, 'min_samples_split': 5}
  0.7617818079181577]
 [{'max_depth': 7, 'min_samples_leaf': 4, 'min_samples_split': 10}
  0.7623014513941999]
 [{'max_depth': 7, 'min_samples_leaf': 4, 'min_samples_split': 15}
  0.762669441644409]
 [{'max_depth': 7, 'min_samples_leaf': 4, 'min_samples_split': 20}
  0.7621455293604104]
 [{'max_depth': 7, 'min_samples_leaf': 6, 'min_samples_split': 5}
  0.7625779235498858]
 [{'max_depth': 7, 'min_samples_leaf': 6, 'min_samples_split': 10}
  0.7625779235498858]
 [{'max_depth': 7, 'min_samples_leaf': 6, 'min_samples_split': 15}
  0.7629887567584489]
  ...
  best parameter is: {'max_depth': 15, 'min_samples_leaf': 8, 'min_samples_split': 5}, best score is: 0.7689744521462432
```
### Tuning max_features
Tuning process with grid search:
```
param_test3= {'max_features': range(1,11)}
gsearch3= GridSearchCV(
    estimator = RandomForestClassifier(n_estimators=400, max_depth=15, min_samples_leaf=8,
                                       min_samples_split=5, random_state=0),
    param_grid =param_test3, scoring='roc_auc',cv=5)
gsearch3.fit(X_train,y_train)
```
```
>>>GridSearchCV(cv=5, estimator=RandomForestClassifier(max_depth=15, min_samples_leaf=8, min_samples_split=5, n_estimators=400, random_state=0), param_grid={'max_features': range(1, 11)}, scoring='roc_auc')
```
Call function show_gridsearch_result() to print and plot parameter and auc score:
```
show_gridsearch_result(gsearch3)
```
```
>>> [[{'max_features': 1} 0.764741271311426]
 [{'max_features': 2} 0.7682340132779089]
 [{'max_features': 3} 0.7689744521462432]
 [{'max_features': 4} 0.7667355620143447]
 [{'max_features': 5} 0.7672077295343797]
 [{'max_features': 6} 0.7665610190521948]
 [{'max_features': 7} 0.7654064050016229]
 [{'max_features': 8} 0.7653961646523135]
 [{'max_features': 9} 0.7643726274382329]
 [{'max_features': 10} 0.7642923926238014]]
best parameter is: {'max_features': 3}, best score is: 0.7689744521462432
```
![Image 9](/v_source/final-9.png)

### ROC Curve
Build final model, predict the outcome and plot ROC curve:
```
rfc = RandomForestClassifier(n_estimators=400, max_depth=15, min_samples_leaf=8,
                            min_samples_split=5, max_features=3, random_state=0)
rf = rfc.fit(X_train,y_train)
predicted_prob_rf = rf.predict_proba(X_test)
```
Call show_auc_roc() to calculate AUC and plot ROC:
```
show_auc_roc(y_test, predicted_prob_rf)
```
![Image 10](/v_source/final-10.png)

### Shap
```
import shap
# explain the model's predictions using SHAP
# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)
explainer = shap.Explainer(rfc)
shap_values = explainer(X_train)
# visualize the first prediction's explanation
```
```
shap.plots.waterfall(shap_values[0][:,1])
```
![Image 11](/v_source/final-11.png)

### - XGBoost
### Tuning n_estimator
```
# Tuning process with grid search
param_test1_xgboost= {'n_estimators':range(1,40,2)}
gsearch1_xgboost= GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,eval_metric="auc"),
                       param_grid = param_test1_xgboost, scoring='roc_auc',cv=5)
gsearch1_xgboost.fit(X_train,y_train)
# call function show_gridsearch_result() to print and plot parameter and auc score
```
```
show_gridsearch_result(gsearch1_xgboost)
```
```
>>> [[{'n_estimators': 1} 0.7199517453556522]
 [{'n_estimators': 3} 0.7404804396638206]
 [{'n_estimators': 5} 0.7496073527990944]
 [{'n_estimators': 7} 0.7540313284010651]
 [{'n_estimators': 9} 0.7575178167729859]
 [{'n_estimators': 11} 0.7563399687791554]
 [{'n_estimators': 13} 0.7568203965748099]
 [{'n_estimators': 15} 0.7578005801089198]
 [{'n_estimators': 17} 0.7582267579286082]
 [{'n_estimators': 19} 0.7567001813890062]
 [{'n_estimators': 21} 0.7563721582522929]
 ...
 best parameter is: {'n_estimators': 17}, best score is: 0.7582267579286082
```
![Image 12](/v_source/final-12.png)

### Tuning max_depth
```
# tuning process with grid search
param_test2_xgboost= {'n_estimators':range(1,60,2),
                      'max_depth': range(3,19,2),
                      'min_child_weight':range(1,5,1),
                      'learning_rate':np.linspace(0.05,1,20)}
gsearch2_xgboost= GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,eval_metric="auc",n_estimators=17),
                       param_grid = param_test2_xgboost, scoring='roc_auc',cv=5)
gsearch2_xgboost.fit(X_train,y_train)
# call function show_gridsearch_result() to print and plot parameter and auc score
show_gridsearch_result(gsearch2_xgboost)
```
```
>>> [[{'max_depth': 3, 'min_child_weight': 1} 0.7608385734721045]
 [{'max_depth': 3, 'min_child_weight': 2} 0.7609029367142792]
 [{'max_depth': 3, 'min_child_weight': 3} 0.7610673133787139]
 [{'max_depth': 3, 'min_child_weight': 4} 0.7610085531560067]
 [{'max_depth': 5, 'min_child_weight': 1} 0.7621128673292725]
 [{'max_depth': 5, 'min_child_weight': 2} 0.7636310029457476]
 [{'max_depth': 5, 'min_child_weight': 3} 0.7594084179357224]
 [{'max_depth': 5, 'min_child_weight': 4} 0.7623356815777657]
 [{'max_depth': 7, 'min_child_weight': 1} 0.7521048011446613]
 ...
best parameter is: {'max_depth': 5, 'min_child_weight': 2}, best score is: 0.7636310029457476
```
### Tuning max_depth
```
# tuning process with grid search
param_test3_xgboost= {'learning_rate':np.linspace(0.05,1,20)}
gsearch3_xgboost= GridSearchCV(estimator = xgb.XGBClassifier(
    use_label_encoder=False,eval_metric="auc",n_estimators=17,max_depth=5,min_child_weight=2),
                               param_grid = param_test3_xgboost, scoring='roc_auc',cv=5)
gsearch3_xgboost.fit(X_train,y_train)
# call function show_gridsearch_result() to print and plot parameter and auc score
show_gridsearch_result(gsearch3_xgboost)
```
```
>>> [[{'learning_rate': 0.05} 0.7442803577312775]
 [{'learning_rate': 0.1} 0.7547153703139151]
 [{'learning_rate': 0.15} 0.7621146674232993]
 [{'learning_rate': 0.2} 0.7617758015711907]
 [{'learning_rate': 0.25} 0.7651121981727782]
 [{'learning_rate': 0.3} 0.7636310029457476]
 ...
 best parameter is: {'learning_rate': 0.25}, best score is: 0.7651121981727782
```
![Image 13](/v_source/final-13.png)

### Build final model, predict the outcome and plot ROC curve
```
# build final model and predict the outcome
xgboostc = xgb.XGBClassifier(use_label_encoder=False,eval_metric="auc",n_estimators=17,
                   max_depth=5,min_child_weight=2)
xgboost = xgboostc.fit(X_train,y_train)
predicted_prob_xgboost = xgboost.predict_proba(X_test)
# call show_auc_roc() to calculate AUC and plot ROC
show_auc_roc(y_test, predicted_prob_xgboost)
```
![Image 14](/v_source/final-14.png)

### Shap
```
# explain the model's predictions using SHAP
# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)
explainer = shap.Explainer(xgboostc)
shap_values = explainer(X_train)
# visualize the first prediction's explanation
shap.plots.waterfall(shap_values[0])
```
![Image 15](/v_source/final-15.png)

### Save model
```
import joblib
joblib.dump(rf, 'rf.joblib')
joblib.dump(xgboost, 'xgboost.joblib')
```
```
>>> ['xgboost.joblib']
```
### Save y_test and predicted_probabilities to csv file
```
pd.DataFrame(np.column_stack([y_test,predicted_prob_rf[:,1],predicted_prob_xgboost[:,1]])).to_csv('ROC_rf_xgboost.csv',index=False)
```

## 3. Web Application
```
import streamlit as st
import model
import pandas as pd
```
```
def user_input():
    """User input variables"""
    st.sidebar.title("Please select the value accordingly.")

    base_excess = st.sidebar.slider('Base Excess', -35.0, 45.0, -35.0, 0.1)
    anion_gap = st.sidebar.slider('Anion Gap', -5.0, 50.0, -5.0, 0.1)
    chloride = st.sidebar.slider('Chloride Level', 60.0, 160.0, 60.0, 0.1)
    creatinine = st.sidebar.slider('Creatinine Level', 0.0, 25.0, 0.0, 0.1)
    potassium = st.sidebar.slider('Potassium Level', 1.0, 20.0, 1.0, 0.1)
    sodium = st.sidebar.slider('Sodium Level', 100.0, 175.0, 100.0, 0.1)
    urea_nitrogen = st.sidebar.slider('Urea nitrogen', 0.0, 220.0, 0.0, 0.1)
    rdw = st.sidebar.slider('Red Cell Distribution Width', 0.0, 35.0, 0.0, 0.1)
    white_cell = st.sidebar.slider('White blood cells', 0.0, 450.0, 0.0, 0.1)
    icu_los = st.sidebar.slider('Length of Stay', 0.0, 265.0, 0.0, 0.1)

    user_info = {
        "Base Excess": base_excess,
        "Anion Gap": anion_gap,
        "Chloride Level": chloride,
        "Creatinine Level": creatinine,
        "Potassium Level": potassium,
        "Sodium Level": sodium,
        "Urea Nitrogen": urea_nitrogen,
        "Red Cell Distribution Width": rdw,
        "White Blood Cells": white_cell,
        "ICU Los": icu_los
    }
    return pd.DataFrame(user_info, index=[0])


st.title("Prediction of Patient's Risk of Death based on Lab Test after Entering ICU")
st.subheader("Variable Used for Death Risk Prediction:")
st.text("""
    - Base Excess: the amount of excess or insufficient level of bicarbonate in the system.
    - Anion Gap: a measurement of the difference-or gap-between the negatively charged and positively charged electrolytes.
    - Chloride: measures the amount of chloride in your blood.
    - Creatinine: a chemical compound left over from energy-producing processes in your muscles.
    - Potassium: measures the amount of potassium in your blood.
    - Sodium: measures the amount of sodium in your blood.
    - Urea Nitrogen: reveals important information about how well your kidneys are working.
    - Red Cell Distribution Width (RDW): a measurement of the range in the volume and size of your red blood cells.
    - White Blood Cells: measures the number of white blood cells in your body.
    - icu_los: ICU length of stay.
    """)
user_input = user_input()
st.subheader("User Input: Blood Test Variables")
st.write(user_input)
st.subheader("Prediction Outcome: ")
prediction = model.death_risk_predict(user_input.iloc[0, 0], user_input.iloc[0, 1], user_input.iloc[0, 2],
                                      user_input.iloc[0, 3], user_input.iloc[0, 4], user_input.iloc[0, 5],
                                      user_input.iloc[0, 6], user_input.iloc[0, 7], user_input.iloc[0, 8],
                                      user_input.iloc[0, 9])
st.write("The risk of death: ", prediction)
st.subheader("Choose the model to view the ROC Curve:")
figure_selection = ['Logistic', 'Linear SVM', 'Poly SVM', 'RBF SVM', 'Random Forest', 'XGBoost']
st.set_option('deprecation.showPyplotGlobalUse', False)
x_input = st.selectbox("Select", figure_selection)
figure = model.ROC_plot(x_input)
st.pyplot(figure)
```

