---
layout: post
title:  "Final Project"
date:   2021-11-24
excerpt: "Prediction of Patient's Risk of Death based on Lab Test after Entering ICU"
tag: [post]
comments: true
---


##  Prediction of Patient's Risk of Death based on Lab Test after Entering ICU

## Dataset

[MIMIC](https://mimic.mit.edu/) dataset is chosen. This dataset includes the unidentified health data related to approximately 60,000 ICU admitted patients (53,432 adult patients and 8,100 neonatal patients) from June 2001 to October 2012. The data used include variables from LABEVENTS.csv, ICUSTAYS.csv, ADMISSIONS.csv, D_LABITEMS.csv and PATIENTS.csv.

Outcome variable (binary):

- `flag`: alive or death.

Predictor variables:

- `Base Excess`: the amount of excess or insufficient level of bicarbonate in the system.
- `Anion Gap`: a measurement of the difference-or gap-between the negatively charged and positively charged electrolytes.
- `Chloride`: measures the amount of chloride in your blood.
- `Creatinine`: a chemical compound left over from energy-producing processes in your muscles.
- `Potassium`: measures the amount of potassium in your blood.
- `Sodium`: measures the amount of sodium in your blood.
- `Urea Nitrogen`: reveals important information about how well your kidneys are working.
- `Red Cell Distribution Width (RDW)`: a measurement of the range in the volume and size of your red blood cells.
- `White Blood Cells`: measures the number of white blood cells in your body.
- `icu_los`: ICU length of stay

## Object
The aim of our project is to predict the risk of death based on blood test after entering the ICU. To achieve it, our work mainly includes three parts:
- Data Preprocessing
- ML Algorithm
- Web Application


## 1.  Data Preprocessing

### Library

```
import pandas as pd
import numpy as np
from sklearn import svm, metrics
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import joblib
```
### Load datasets
```
icustays = pd.read_csv('ICUSTAYS.csv')
labevents = pd.read_csv('LABEVENTS.csv')
admissions = pd.read_csv('ADMISSIONS.csv')
d_labitems = pd.read_csv('D_LABITEMS.csv')
patients = pd.read_csv('PATIENTS.csv')
```
### Icustays Processing
Firstly, for icustays processing, we obtain the time of each patient entering and leaving ICU for the first time, and the length of stay in ICU.

Change the "INTIME" and "OUTTIME" to datetime format, sort the icustays datafrmae by "INTIME".
```
icustays['INTIME'] = pd.to_datetime(icustays['INTIME'])
icustays['OUTTIME'] = pd.to_datetime(icustays['OUTTIME'])
icustays.sort_values('INTIME', inplace = True)
```
Delete rows with missing values in column "OUTTIME".
```
icustays.drop(icustays[np.isnan(icustays['OUTTIME'])].index, axis = 0, inplace = True)
```
Obtain the intime, outtime for each patient to enter the icu for the first time. Due to the sorting of intime, the data in the first row of each patient is the data entered into ICU for the first time.
```
time_dict = {}
for i in icustays.index:
    if icustays['SUBJECT_ID'][i] not in time_dict.keys():
        time_dict[icustays['SUBJECT_ID'][i]] = {'INTIME' : icustays['INTIME'][i], 
                                                'OUTTIME' : icustays['OUTTIME'][i]}
```
### Labevents processing
Secondly, we implement labevents processing. Each row of the datasets represents the value of a certain item tested by a certain patient. A patient may have multiple tests for one item. For a certain item, we want to obtain the value of the patient's first test during the first time entering ICU.

Only keep patients who have been in ICU
```
labevents_icu = labevents[[(sub_id in time_dict.keys()) for sub_id in labevents['SUBJECT_ID']]]
```
Change the "CHARTTIME" to datetime format, sort the labevents_icu datafrmae by "INTIME".
```
labevents_icu['CHARTTIME'] = pd.to_datetime(labevents_icu['CHARTTIME'])
labevents_icu.sort_values('CHARTTIME', inplace = True)
```
For every patient and every item, obtain the first testing value during his first entry into ICU.
```
%%time

itemid_dict = {}
require_index = {}
for i in labevents_icu.index:
    if labevents_icu['CHARTTIME'][i] > time_dict[labevents_icu['SUBJECT_ID'][i]]['INTIME'] and labevents_icu['CHARTTIME'][i] < time_dict[labevents_icu['SUBJECT_ID'][i]]['OUTTIME']:
        if labevents_icu['SUBJECT_ID'][i] not in itemid_dict.keys():
            itemid_dict[labevents_icu['SUBJECT_ID'][i]] = []
        if labevents_icu['ITEMID'][i] not in itemid_dict[labevents_icu['SUBJECT_ID'][i]]:
            itemid_dict[labevents_icu['SUBJECT_ID'][i]].append(labevents_icu['ITEMID'][i])
            require_index[i] = 0
```
```
>>> CPU times: user 13min 9s, sys: 4.19 s, total: 13min 13s
>>> Wall time: 13min 17s
```
Select required rows:
```
labevents_unique = labevents_icu[[(index in require_index.keys()) for index in labevents_icu.index]]
```
Obtain the HADM_ID dict, in labevents_unqiue, for each patient, the HADM_ID should be unique.
```
HADM_dict = {}
for i in labevents_unique.index:
    if labevents_unique['SUBJECT_ID'][i] not in HADM_dict.keys():
        HADM_dict[labevents_unique['SUBJECT_ID'][i]] = labevents_unique['HADM_ID'][i]
```
Since the labevents are unique for each subject, we can use pd.pivot to convert the labevents_unique dataframe.

Convert labevents_unique:
```
convert = labevents_unique.pivot(index='SUBJECT_ID',columns='ITEMID',values='VALUE')
```
Since there are too many nan in convert, we want to keep the columns with missing values less than 20000, then delete the rows that has missing values in these columns.
```
drop_list = []
for column in convert.columns:
    if sum(pd.isna(convert[column])) > 20000:
        drop_list.append(column)
convert_dropna = convert.drop(drop_list, axis = 1).dropna(axis = 0, how = 'any')
```
Some of the value cannot be transfered to float, therefore, we want to delete the rows with these string values.
```
def isfloat(i):
    try:
        float(i)
        return True
    except:
        return False
    
drop_index = []
for i in convert_dropna.index:
    for column in convert_dropna.columns:
        if not isfloat(convert_dropna[column][i]):
            drop_index.append(i)
            
convert_float = convert_dropna.drop(drop_index,axis = 0)
```
### Admissions processing
We obtain the target variable and Admittime from admissions datasets.

Obtain the HADM_ID, flag, admittime for each patient:
```
admissions['ADMITTIME'] = pd.to_datetime(admissions['ADMITTIME'])
admissions = admissions.sort_values('ADMITTIME')
HADM_dict = {}
admittime = {}
flag_dict = {}
for i in admissions.index:
    if admissions['SUBJECT_ID'][i] not in HADM_dict.keys():
        HADM_dict[admissions['SUBJECT_ID'][i]] = admissions['HADM_ID'][i]
        admittime[admissions['SUBJECT_ID'][i]] = admissions['ADMITTIME'][i]
        flag_dict[admissions['SUBJECT_ID'][i]] = admissions['HOSPITAL_EXPIRE_FLAG'][i]
```
### Patients processing
This dataset includes patients information, we want to get the date of birthday and gender for each patient.
```
patients['GENDER'] = patients['GENDER'].replace({'F':0, 'M':1})
patients['DOB'] = pd.to_datetime(patients['DOB'])
```
Obtain the gender_dict and age_dict:
```
gender_dict = {}
age_dict = {}
for i in patients.index:
    gender_dict[patients['SUBJECT_ID'][i]] = patients['GENDER'][i]
    age_dict[patients['SUBJECT_ID'][i]] = admittime[patients['SUBJECT_ID'][i]].year - patients['DOB'][i].year
```
### Combination
Based on subject_id, combine the variable we get.

Based on the HADM_dict obtained from admissions, obtain icu_los from icustays dataframe. One hospitalization may enter the ICU multiple times, and the length needs to be accumulated
```
icu_los = {}
for i in icustays.index:
    if icustays['HADM_ID'][i] == HADM_dict[icustays['SUBJECT_ID'][i]]:
        if icustays['SUBJECT_ID'][i] not in icu_los.keys():
            icu_los[icustays['SUBJECT_ID'][i]] = 0
        icu_los[icustays['SUBJECT_ID'][i]] += icustays['LOS'][i]
```
```
combine = convert_float[[(index in icu_los.keys()) for index in convert_float.index]]
```
```
combine['age'] = -1.0
combine['gender'] = -1
combine['icu_los'] = -1.0
combine['flag'] = -1

for i in combine.index:
    combine.age[i] = age_dict[i]
    combine.gender[i] = gender_dict[i]
    combine.icu_los[i] = time_dict[i]['icu_los']
    combine.flag[i] = flag_dict[i]
```
### Variables selection
There are too many variables, we want to choose 10 variables for the next part of data analysis. Therefore, we use a simple logistic regression to select variables.

Map the data to the range of 0-1:
```
combine_nor = combine.copy()
for column in combine_nor.columns:
    combine_nor[column] = combine_nor[column].astype('float') - min(combine_nor[column].astype('float'))
    combine_nor[column] = combine_nor[column] / max(combine_nor[column])
```
Logistic regression:
```
x_column = combine_nor.columns[:-1]
x = combine_nor[x_column]
y = combine_nor['flag']
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)
clf = LogisticRegression(random_state=0, class_weight = 'balanced').fit(X_train, y_train)
clf.score(X_test,y_test)
```
```
>>> 0.6987647389107243
```
The importance of variables:
```
clf.coef_
```
```
>>> array([[-2.1758029 , -2.73706951,  1.55998412,  0.54184627, -0.3229467 ,
         3.59556658, -0.37894219, -1.12112914, -4.30788353, -4.252456  ,
         1.81547318, -1.923065  , -0.50033385, -2.18868785,  3.00685438,
         2.89995361, -0.29202966,  0.66445185,  1.32347575,  1.32839141,
        -2.36666072,  1.98036039, -0.87288612,  1.21127145,  0.69200924,
         4.90876271,  0.67380674,  2.69264174,  1.2553999 , -0.0210873 ,
         1.73804051]])
```
```
x_column
```
```
>>> Index([    50802,     50804,     50818,     50820,     50821,     50868,
           50882,     50893,     50902,     50912,     50931,     50960,
           50970,     50971,     50983,     51006,     51221,     51222,
           51237,     51248,     51249,     51250,     51265,     51274,
           51275,     51277,     51279,     51301,     'age',  'gender',
       'icu_los'],
      dtype='object', name='ITEMID')
```
Drop the variables with low importance:
```
drop_cols = [50820, 50821, 50882, 50970, 51221, 51222, 
             51265, 51275, 51279, 50818, 50893, 50804,
             51274, 51250, 51249, 51248, 51237, 50931, 
             50960,'gender','age']

combine_new = combine.drop(drop_cols, axis = 1)
```
Map the data to the range of 0-1:
```
combine_nor = combine_new.copy()
for column in combine_nor.columns:
    combine_nor[column] = combine_nor[column].astype('float') - min(combine_nor[column].astype('float'))
    combine_nor[column] = combine_nor[column] / max(combine_nor[column])
```
Repeat the logistic regression:
```
x_column = combine_nor.columns[:-1]
x = combine_nor[x_column]
y = combine_nor['flag']
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)
clf = LogisticRegression(random_state=0, class_weight = 'balanced').fit(X_train, y_train)
clf.score(X_test,y_test)
```
```
>>> 0.7063447501403706
```
```
clf.coef_
```
```
>>> array([[-4.95377852,  4.10662256, -3.96803642, -4.65643424, -2.22710977,
         3.08866454,  3.54293   ,  5.54482311,  2.77263174,  1.53964616]])

```
We can see that these ten variables have high importance. Therefore, we choose these ten variables for the next part of data analysis.

### Convert itemid and save dataframe
Transfer item id to item name:
```
itemid_dict = {}
for i in d_labitems.index:
    if d_labitems['ITEMID'][i] in combine_new.columns:
        itemid_dict[d_labitems['ITEMID'][i]] = d_labitems['LABEL'][i]
        
final_dataset = combine_new.rename(columns = itemid_dict)
final_dataset
```
Save to csv file:
```
final_dataset.to_csv('death_risk_predict.csv')
```
